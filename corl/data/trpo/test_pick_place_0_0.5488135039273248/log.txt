Logging to /root/code/ProMP/corl/data/trpo/test_pick_place_0_0.5488135039273248

 ---------------- Iteration 0 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 0              |
| ItrTime                 | 120            |
| LossAfter               | -0.013038878   |
| LossBefore              | -2.2443989e-09 |
| MeanKL                  | 0.007239829    |
| MeanKLBefore            | 5.9138983e-09  |
| Step_0-AverageDiscou... | -24.3          |
| Step_0-AveragePolicyStd | 1.0            |
| Step_0-AverageReturn    | -57.8          |
| Step_0-EnvExecTime      | 9.39           |
| Step_0-MaxReturn        | 81.1           |
| Step_0-MinReturn        | -89.8          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.95           |
| Step_0-StdReturn        | 19.4           |
| Step_1-AverageDiscou... | -19.6          |
| Step_1-AveragePolicyStd | 1.0006222      |
| Step_1-AverageReturn    | -44.5          |
| Step_1-EnvExecTime      | 10.3           |
| Step_1-MaxReturn        | 33.6           |
| Step_1-MinReturn        | -78.9          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 34.1           |
| Step_1-StdReturn        | 14.1           |
| Time                    | 120            |
| Time-InnerStep          | 4.5            |
| Time-MAMLSteps          | 49.4           |
| Time-OuterStep          | 49.4           |
| Time-SampleProc         | 0.572          |
| Time-Sampling           | 64.9           |
| Time-TotalInner         | 70             |
| dLoss                   | 0.0130388765   |
| n_timesteps             | 80000          |
--------------------------------------------

 ---------------- Iteration 1 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
------------------------------------------
| Itr                     | 1            |
| ItrTime                 | 66.3         |
| LossAfter               | -0.009549232 |
| LossBefore              | 1.017004e-09 |
| MeanKL                  | 0.0082908    |
| MeanKLBefore            | 0.0          |
| Step_0-AverageDiscou... | -18.9        |
| Step_0-AveragePolicyStd | 0.9945376    |
| Step_0-AverageReturn    | -43.1        |
| Step_0-EnvExecTime      | 8.52         |
| Step_0-MaxReturn        | 84.8         |
| Step_0-MinReturn        | -80.5        |
| Step_0-NumTrajs         | 280          |
| Step_0-PolicyExecTime   | 8.55         |
| Step_0-StdReturn        | 17.6         |
| Step_1-AverageDiscou... | -15.4        |
| Step_1-AveragePolicyStd | 0.99468696   |
| Step_1-AverageReturn    | -33.2        |
| Step_1-EnvExecTime      | 10.4         |
| Step_1-MaxReturn        | 123          |
| Step_1-MinReturn        | -78.2        |
| Step_1-NumTrajs         | 280          |
| Step_1-PolicyExecTime   | 32.6         |
| Step_1-StdReturn        | 16.8         |
| Time                    | 186          |
| Time-InnerStep          | 0.0782       |
| Time-MAMLSteps          | 3.62         |
| Time-OuterStep          | 3.62         |
| Time-SampleProc         | 0.318        |
| Time-Sampling           | 62.2         |
| Time-TotalInner         | 62.6         |
| dLoss                   | 0.009549233  |
| n_timesteps             | 160000       |
------------------------------------------

 ---------------- Iteration 2 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 2              |
| ItrTime                 | 84.8           |
| LossAfter               | -0.0106850965  |
| LossBefore              | 2.9273686e-09  |
| MeanKL                  | 0.007708952    |
| MeanKLBefore            | -3.1664968e-09 |
| Step_0-AverageDiscou... | -13.5          |
| Step_0-AveragePolicyStd | 0.9883511      |
| Step_0-AverageReturn    | -28.3          |
| Step_0-EnvExecTime      | 17             |
| Step_0-MaxReturn        | 1.13e+03       |
| Step_0-MinReturn        | -60.1          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 11.8           |
| Step_0-StdReturn        | 69.8           |
| Step_1-AverageDiscou... | -13.9          |
| Step_1-AveragePolicyStd | 0.98828137     |
| Step_1-AverageReturn    | -28.7          |
| Step_1-EnvExecTime      | 12.4           |
| Step_1-MaxReturn        | -17.8          |
| Step_1-MinReturn        | -55            |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 36.1           |
| Step_1-StdReturn        | 6.99           |
| Time                    | 271            |
| Time-InnerStep          | 0.134          |
| Time-MAMLSteps          | 3.81           |
| Time-OuterStep          | 3.81           |
| Time-SampleProc         | 0.7            |
| Time-Sampling           | 80.1           |
| Time-TotalInner         | 80.9           |
| dLoss                   | 0.010685099    |
| n_timesteps             | 240000         |
--------------------------------------------

 ---------------- Iteration 3 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 3              |
| ItrTime                 | 65.7           |
| LossAfter               | -0.0026820647  |
| LossBefore              | -1.7461854e-09 |
| MeanKL                  | 0.005243738    |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | -13.3          |
| Step_0-AveragePolicyStd | 1.0020409      |
| Step_0-AverageReturn    | -27.3          |
| Step_0-EnvExecTime      | 8.58           |
| Step_0-MaxReturn        | -16.6          |
| Step_0-MinReturn        | -54            |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.32           |
| Step_0-StdReturn        | 6.12           |
| Step_1-AverageDiscou... | -12.3          |
| Step_1-AveragePolicyStd | 1.001818       |
| Step_1-AverageReturn    | -24.1          |
| Step_1-EnvExecTime      | 10.4           |
| Step_1-MaxReturn        | -17            |
| Step_1-MinReturn        | -38            |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 32.1           |
| Step_1-StdReturn        | 3.31           |
| Time                    | 337            |
| Time-InnerStep          | 0.074          |
| Time-MAMLSteps          | 3.57           |
| Time-OuterStep          | 3.57           |
| Time-SampleProc         | 0.345          |
| Time-Sampling           | 61.7           |
| Time-TotalInner         | 62.2           |
| dLoss                   | 0.002682063    |
| n_timesteps             | 320000         |
--------------------------------------------

 ---------------- Iteration 4 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 4              |
| ItrTime                 | 71.4           |
| LossAfter               | -0.005341881   |
| LossBefore              | -1.3560058e-09 |
| MeanKL                  | 0.00910853     |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | -12.8          |
| Step_0-AveragePolicyStd | 1.0192987      |
| Step_0-AverageReturn    | -25.6          |
| Step_0-EnvExecTime      | 9.01           |
| Step_0-MaxReturn        | -17.1          |
| Step_0-MinReturn        | -46.5          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.32           |
| Step_0-StdReturn        | 4.37           |
| Step_1-AverageDiscou... | -12.2          |
| Step_1-AveragePolicyStd | 1.0185795      |
| Step_1-AverageReturn    | -24            |
| Step_1-EnvExecTime      | 11.5           |
| Step_1-MaxReturn        | -16            |
| Step_1-MinReturn        | -36.4          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 36.4           |
| Step_1-StdReturn        | 3.07           |
| Time                    | 408            |
| Time-InnerStep          | 0.0879         |
| Time-MAMLSteps          | 3.5            |
| Time-OuterStep          | 3.5            |
| Time-SampleProc         | 0.249          |
| Time-Sampling           | 67.5           |
| Time-TotalInner         | 67.9           |
| dLoss                   | 0.0053418796   |
| n_timesteps             | 400000         |
--------------------------------------------

 ---------------- Iteration 5 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 5              |
| ItrTime                 | 66.5           |
| LossAfter               | -0.004724114   |
| LossBefore              | 7.328177e-10   |
| MeanKL                  | 0.007474059    |
| MeanKLBefore            | -3.1664968e-09 |
| Step_0-AverageDiscou... | -12.6          |
| Step_0-AveragePolicyStd | 0.9847259      |
| Step_0-AverageReturn    | -24.9          |
| Step_0-EnvExecTime      | 8.94           |
| Step_0-MaxReturn        | -17.7          |
| Step_0-MinReturn        | -41.7          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.51           |
| Step_0-StdReturn        | 4.11           |
| Step_1-AverageDiscou... | -12.1          |
| Step_1-AveragePolicyStd | 0.9852364      |
| Step_1-AverageReturn    | -23.5          |
| Step_1-EnvExecTime      | 10             |
| Step_1-MaxReturn        | -17.6          |
| Step_1-MinReturn        | -35.9          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 32.7           |
| Step_1-StdReturn        | 2.89           |
| Time                    | 475            |
| Time-InnerStep          | 0.0757         |
| Time-MAMLSteps          | 3.31           |
| Time-OuterStep          | 3.31           |
| Time-SampleProc         | 0.48           |
| Time-Sampling           | 62.6           |
| Time-TotalInner         | 63.1           |
| dLoss                   | 0.004724115    |
| n_timesteps             | 480000         |
--------------------------------------------

 ---------------- Iteration 6 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 6             |
| ItrTime                 | 71.3          |
| LossAfter               | -0.0030275795 |
| LossBefore              | 1.9460207e-09 |
| MeanKL                  | 0.008512157   |
| MeanKLBefore            | 0.0           |
| Step_0-AverageDiscou... | -12.4         |
| Step_0-AveragePolicyStd | 0.9905759     |
| Step_0-AverageReturn    | -24.6         |
| Step_0-EnvExecTime      | 10.3          |
| Step_0-MaxReturn        | -18.6         |
| Step_0-MinReturn        | -36.1         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.77          |
| Step_0-StdReturn        | 3.08          |
| Step_1-AverageDiscou... | -11.7         |
| Step_1-AveragePolicyStd | 0.9901744     |
| Step_1-AverageReturn    | -22.5         |
| Step_1-EnvExecTime      | 10.9          |
| Step_1-MaxReturn        | -16           |
| Step_1-MinReturn        | -29.3         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 32.9          |
| Step_1-StdReturn        | 2.52          |
| Time                    | 546           |
| Time-InnerStep          | 0.078         |
| Time-MAMLSteps          | 5.22          |
| Time-OuterStep          | 5.22          |
| Time-SampleProc         | 0.485         |
| Time-Sampling           | 65.5          |
| Time-TotalInner         | 66.1          |
| dLoss                   | 0.0030275814  |
| n_timesteps             | 560000        |
-------------------------------------------

 ---------------- Iteration 7 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 7              |
| ItrTime                 | 66.4           |
| LossAfter               | -0.0036373057  |
| LossBefore              | -6.882912e-11  |
| MeanKL                  | 0.007658069    |
| MeanKLBefore            | -3.1664968e-09 |
| Step_0-AverageDiscou... | -12.3          |
| Step_0-AveragePolicyStd | 1.0057393      |
| Step_0-AverageReturn    | -24.3          |
| Step_0-EnvExecTime      | 8.83           |
| Step_0-MaxReturn        | -16.8          |
| Step_0-MinReturn        | -34.7          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.61           |
| Step_0-StdReturn        | 2.9            |
| Step_1-AverageDiscou... | -11.7          |
| Step_1-AveragePolicyStd | 1.0055861      |
| Step_1-AverageReturn    | -22.5          |
| Step_1-EnvExecTime      | 9.62           |
| Step_1-MaxReturn        | -16.6          |
| Step_1-MinReturn        | -34.1          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 31.7           |
| Step_1-StdReturn        | 2.25           |
| Time                    | 612            |
| Time-InnerStep          | 0.0883         |
| Time-MAMLSteps          | 5.08           |
| Time-OuterStep          | 5.08           |
| Time-SampleProc         | 0.222          |
| Time-Sampling           | 60.9           |
| Time-TotalInner         | 61.3           |
| dLoss                   | 0.0036373057   |
| n_timesteps             | 640000         |
--------------------------------------------

 ---------------- Iteration 8 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 4
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 8              |
| ItrTime                 | 73.9           |
| LossAfter               | -0.0029140227  |
| LossBefore              | 5.470855e-10   |
| MeanKL                  | 0.0067227716   |
| MeanKLBefore            | -5.5879357e-10 |
| Step_0-AverageDiscou... | -12.2          |
| Step_0-AveragePolicyStd | 1.0220109      |
| Step_0-AverageReturn    | -24.1          |
| Step_0-EnvExecTime      | 10.9           |
| Step_0-MaxReturn        | -18.7          |
| Step_0-MinReturn        | -34.3          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 10.7           |
| Step_0-StdReturn        | 2.47           |
| Step_1-AverageDiscou... | -11.4          |
| Step_1-AveragePolicyStd | 1.0214939      |
| Step_1-AverageReturn    | -21.8          |
| Step_1-EnvExecTime      | 11.1           |
| Step_1-MaxReturn        | -16.9          |
| Step_1-MinReturn        | -27.5          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 33.8           |
| Step_1-StdReturn        | 2.19           |
| Time                    | 686            |
| Time-InnerStep          | 0.0941         |
| Time-MAMLSteps          | 4.6            |
| Time-OuterStep          | 4.6            |
| Time-SampleProc         | 0.333          |
| Time-Sampling           | 68.8           |
| Time-TotalInner         | 69.2           |
| dLoss                   | 0.0029140231   |
| n_timesteps             | 720000         |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 9             |
| ItrTime                 | 77.8          |
| LossAfter               | -0.0039174194 |
| LossBefore              | 1.6816137e-09 |
| MeanKL                  | 0.008940974   |
| MeanKLBefore            | 1.0430813e-08 |
| Step_0-AverageDiscou... | -12.3         |
| Step_0-AveragePolicyStd | 1.0014899     |
| Step_0-AverageReturn    | -24.3         |
| Step_0-EnvExecTime      | 8.83          |
| Step_0-MaxReturn        | -17.7         |
| Step_0-MinReturn        | -36.3         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.56          |
| Step_0-StdReturn        | 2.74          |
| Step_1-AverageDiscou... | -11.4         |
| Step_1-AveragePolicyStd | 1.0013338     |
| Step_1-AverageReturn    | -21.7         |
| Step_1-EnvExecTime      | 14.7          |
| Step_1-MaxReturn        | -16.6         |
| Step_1-MinReturn        | -27.4         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 39.1          |
| Step_1-StdReturn        | 2.14          |
| Time                    | 764           |
| Time-InnerStep          | 0.0857        |
| Time-MAMLSteps          | 3.88          |
| Time-OuterStep          | 3.88          |
| Time-SampleProc         | 0.31          |
| Time-Sampling           | 73.5          |
| Time-TotalInner         | 73.9          |
| dLoss                   | 0.003917421   |
| n_timesteps             | 800000        |
-------------------------------------------

 ---------------- Iteration 10 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 10             |
| ItrTime                 | 71.8           |
| LossAfter               | -0.0031324732  |
| LossBefore              | -2.1571203e-09 |
| MeanKL                  | 0.009868713    |
| MeanKLBefore            | 0.0            |
| Step_0-AverageDiscou... | -12.8          |
| Step_0-AveragePolicyStd | 1.0211748      |
| Step_0-AverageReturn    | -25.7          |
| Step_0-EnvExecTime      | 11.1           |
| Step_0-MaxReturn        | -20.2          |
| Step_0-MinReturn        | -34.9          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.58           |
| Step_0-StdReturn        | 2.83           |
| Step_1-AverageDiscou... | -11.2          |
| Step_1-AveragePolicyStd | 1.020893       |
| Step_1-AverageReturn    | -21.5          |
| Step_1-EnvExecTime      | 11             |
| Step_1-MaxReturn        | -15.9          |
| Step_1-MinReturn        | -29.8          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 33.2           |
| Step_1-StdReturn        | 2.29           |
| Time                    | 836            |
| Time-InnerStep          | 0.087          |
| Time-MAMLSteps          | 3.69           |
| Time-OuterStep          | 3.68           |
| Time-SampleProc         | 0.381          |
| Time-Sampling           | 67.6           |
| Time-TotalInner         | 68.1           |
| dLoss                   | 0.003132471    |
| n_timesteps             | 880000         |
--------------------------------------------

 ---------------- Iteration 11 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 11            |
| ItrTime                 | 67.5          |
| LossAfter               | -0.0062220916 |
| LossBefore              | -4.585655e-10 |
| MeanKL                  | 0.008611391   |
| MeanKLBefore            | 4.377216e-09  |
| Step_0-AverageDiscou... | -13.1         |
| Step_0-AveragePolicyStd | 1.004587      |
| Step_0-AverageReturn    | -26.2         |
| Step_0-EnvExecTime      | 9.55          |
| Step_0-MaxReturn        | -20.3         |
| Step_0-MinReturn        | -35.6         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.11          |
| Step_0-StdReturn        | 2.61          |
| Step_1-AverageDiscou... | -10.9         |
| Step_1-AveragePolicyStd | 1.0042717     |
| Step_1-AverageReturn    | -20.8         |
| Step_1-EnvExecTime      | 10.7          |
| Step_1-MaxReturn        | -15.5         |
| Step_1-MinReturn        | -28.5         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 32.5          |
| Step_1-StdReturn        | 2.17          |
| Time                    | 903           |
| Time-InnerStep          | 0.073         |
| Time-MAMLSteps          | 3.88          |
| Time-OuterStep          | 3.88          |
| Time-SampleProc         | 0.405         |
| Time-Sampling           | 63.1          |
| Time-TotalInner         | 63.6          |
| dLoss                   | 0.006222091   |
| n_timesteps             | 960000        |
-------------------------------------------

 ---------------- Iteration 12 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 12             |
| ItrTime                 | 71.7           |
| LossAfter               | -0.005378956   |
| LossBefore              | 6.1307615e-10  |
| MeanKL                  | 0.008769424    |
| MeanKLBefore            | -5.5879354e-09 |
| Step_0-AverageDiscou... | -13.5          |
| Step_0-AveragePolicyStd | 0.98169976     |
| Step_0-AverageReturn    | -26.7          |
| Step_0-EnvExecTime      | 10.9           |
| Step_0-MaxReturn        | -21.3          |
| Step_0-MinReturn        | -35            |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.84           |
| Step_0-StdReturn        | 2.17           |
| Step_1-AverageDiscou... | -10.7          |
| Step_1-AveragePolicyStd | 0.9818164      |
| Step_1-AverageReturn    | -20.2          |
| Step_1-EnvExecTime      | 10.8           |
| Step_1-MaxReturn        | -14.3          |
| Step_1-MinReturn        | -26.7          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 33.2           |
| Step_1-StdReturn        | 2.12           |
| Time                    | 975            |
| Time-InnerStep          | 0.0798         |
| Time-MAMLSteps          | 3.73           |
| Time-OuterStep          | 3.73           |
| Time-SampleProc         | 0.635          |
| Time-Sampling           | 67.2           |
| Time-TotalInner         | 68             |
| dLoss                   | 0.0053789564   |
| n_timesteps             | 1040000        |
--------------------------------------------

 ---------------- Iteration 13 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 13            |
| ItrTime                 | 67            |
| LossAfter               | -0.0065975436 |
| LossBefore              | 2.2954885e-09 |
| MeanKL                  | 0.008024724   |
| MeanKLBefore            | 9.313226e-10  |
| Step_0-AverageDiscou... | -14.2         |
| Step_0-AveragePolicyStd | 0.96126676    |
| Step_0-AverageReturn    | -28.3         |
| Step_0-EnvExecTime      | 9.05          |
| Step_0-MaxReturn        | -22.4         |
| Step_0-MinReturn        | -35.1         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.83          |
| Step_0-StdReturn        | 2.05          |
| Step_1-AverageDiscou... | -10.3         |
| Step_1-AveragePolicyStd | 0.9611471     |
| Step_1-AverageReturn    | -19.5         |
| Step_1-EnvExecTime      | 10.2          |
| Step_1-MaxReturn        | -15           |
| Step_1-MinReturn        | -28.3         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 32.3          |
| Step_1-StdReturn        | 1.87          |
| Time                    | 1.04e+03      |
| Time-InnerStep          | 0.087         |
| Time-MAMLSteps          | 4             |
| Time-OuterStep          | 4             |
| Time-SampleProc         | 0.327         |
| Time-Sampling           | 62.5          |
| Time-TotalInner         | 63            |
| dLoss                   | 0.006597546   |
| n_timesteps             | 1120000       |
-------------------------------------------

 ---------------- Iteration 14 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 14            |
| ItrTime                 | 71.1          |
| LossAfter               | -0.005445926  |
| LossBefore              | -9.228078e-10 |
| MeanKL                  | 0.008510868   |
| MeanKLBefore            | 1.1920929e-08 |
| Step_0-AverageDiscou... | -14.7         |
| Step_0-AveragePolicyStd | 0.9378223     |
| Step_0-AverageReturn    | -29.2         |
| Step_0-EnvExecTime      | 10.7          |
| Step_0-MaxReturn        | -24.6         |
| Step_0-MinReturn        | -36           |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 9.52          |
| Step_0-StdReturn        | 1.84          |
| Step_1-AverageDiscou... | -10.1         |
| Step_1-AveragePolicyStd | 0.9377884     |
| Step_1-AverageReturn    | -19           |
| Step_1-EnvExecTime      | 10.5          |
| Step_1-MaxReturn        | -14.8         |
| Step_1-MinReturn        | -24.2         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 33.6          |
| Step_1-StdReturn        | 1.77          |
| Time                    | 1.11e+03      |
| Time-InnerStep          | 0.0874        |
| Time-MAMLSteps          | 3.89          |
| Time-OuterStep          | 3.89          |
| Time-SampleProc         | 0.531         |
| Time-Sampling           | 66.5          |
| Time-TotalInner         | 67.1          |
| dLoss                   | 0.005445925   |
| n_timesteps             | 1200000       |
-------------------------------------------

 ---------------- Iteration 15 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 4
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 15            |
| ItrTime                 | 66.9          |
| LossAfter               | -0.004585472  |
| LossBefore              | 1.1012669e-09 |
| MeanKL                  | 0.007371884   |
| MeanKLBefore            | -8.195639e-09 |
| Step_0-AverageDiscou... | -15.7         |
| Step_0-AveragePolicyStd | 0.9157257     |
| Step_0-AverageReturn    | -31.3         |
| Step_0-EnvExecTime      | 8.74          |
| Step_0-MaxReturn        | -27.1         |
| Step_0-MinReturn        | -36.2         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.39          |
| Step_0-StdReturn        | 1.8           |
| Step_1-AverageDiscou... | -10           |
| Step_1-AveragePolicyStd | 0.91547227    |
| Step_1-AverageReturn    | -18.9         |
| Step_1-EnvExecTime      | 10.8          |
| Step_1-MaxReturn        | -14.9         |
| Step_1-MinReturn        | -24.8         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 32.3          |
| Step_1-StdReturn        | 1.92          |
| Time                    | 1.18e+03      |
| Time-InnerStep          | 0.0852        |
| Time-MAMLSteps          | 3.79          |
| Time-OuterStep          | 3.79          |
| Time-SampleProc         | 0.457         |
| Time-Sampling           | 62.6          |
| Time-TotalInner         | 63.1          |
| dLoss                   | 0.004585473   |
| n_timesteps             | 1280000       |
-------------------------------------------

 ---------------- Iteration 16 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 3
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 16             |
| ItrTime                 | 72.6           |
| LossAfter               | -0.0038112565  |
| LossBefore              | -5.9746563e-10 |
| MeanKL                  | 0.008066982    |
| MeanKLBefore            | 4.4703485e-09  |
| Step_0-AverageDiscou... | -16.4          |
| Step_0-AveragePolicyStd | 0.896969       |
| Step_0-AverageReturn    | -32.7          |
| Step_0-EnvExecTime      | 11.4           |
| Step_0-MaxReturn        | -27.3          |
| Step_0-MinReturn        | -38.6          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.5            |
| Step_0-StdReturn        | 1.91           |
| Step_1-AverageDiscou... | -9.87          |
| Step_1-AveragePolicyStd | 0.8974289      |
| Step_1-AverageReturn    | -18.6          |
| Step_1-EnvExecTime      | 10.5           |
| Step_1-MaxReturn        | -14.6          |
| Step_1-MinReturn        | -25.8          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 34.7           |
| Step_1-StdReturn        | 1.89           |
| Time                    | 1.25e+03       |
| Time-InnerStep          | 0.0804         |
| Time-MAMLSteps          | 3.84           |
| Time-OuterStep          | 3.84           |
| Time-SampleProc         | 0.373          |
| Time-Sampling           | 68.3           |
| Time-TotalInner         | 68.7           |
| dLoss                   | 0.0038112558   |
| n_timesteps             | 1360000        |
--------------------------------------------

 ---------------- Iteration 17 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 17             |
| ItrTime                 | 79.6           |
| LossAfter               | -0.0059723407  |
| LossBefore              | -2.6985294e-09 |
| MeanKL                  | 0.007297653    |
| MeanKLBefore            | -4.4703485e-09 |
| Step_0-AverageDiscou... | -16.4          |
| Step_0-AveragePolicyStd | 0.9076481      |
| Step_0-AverageReturn    | -32.5          |
| Step_0-EnvExecTime      | 12.4           |
| Step_0-MaxReturn        | -28.5          |
| Step_0-MinReturn        | -38.6          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.63           |
| Step_0-StdReturn        | 1.74           |
| Step_1-AverageDiscou... | -9.64          |
| Step_1-AveragePolicyStd | 0.9079288      |
| Step_1-AverageReturn    | -18.3          |
| Step_1-EnvExecTime      | 12.9           |
| Step_1-MaxReturn        | -14.6          |
| Step_1-MinReturn        | -27.4          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 37.2           |
| Step_1-StdReturn        | 1.7            |
| Time                    | 1.33e+03       |
| Time-InnerStep          | 0.137          |
| Time-MAMLSteps          | 3.73           |
| Time-OuterStep          | 3.73           |
| Time-SampleProc         | 0.91           |
| Time-Sampling           | 74.8           |
| Time-TotalInner         | 75.8           |
| dLoss                   | 0.005972338    |
| n_timesteps             | 1440000        |
--------------------------------------------

 ---------------- Iteration 18 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 18             |
| ItrTime                 | 70.1           |
| LossAfter               | -0.0036359485  |
| LossBefore              | -2.311099e-09  |
| MeanKL                  | 0.0061556622   |
| MeanKLBefore            | -1.4901161e-08 |
| Step_0-AverageDiscou... | -16.5          |
| Step_0-AveragePolicyStd | 0.89545137     |
| Step_0-AverageReturn    | -32.8          |
| Step_0-EnvExecTime      | 11             |
| Step_0-MaxReturn        | -28            |
| Step_0-MinReturn        | -37.7          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.42           |
| Step_0-StdReturn        | 1.52           |
| Step_1-AverageDiscou... | -9.52          |
| Step_1-AveragePolicyStd | 0.89512795     |
| Step_1-AverageReturn    | -18            |
| Step_1-EnvExecTime      | 10.2           |
| Step_1-MaxReturn        | -13.5          |
| Step_1-MinReturn        | -25.3          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 33.1           |
| Step_1-StdReturn        | 1.93           |
| Time                    | 1.4e+03        |
| Time-InnerStep          | 0.0737         |
| Time-MAMLSteps          | 3.39           |
| Time-OuterStep          | 3.39           |
| Time-SampleProc         | 0.423          |
| Time-Sampling           | 66.2           |
| Time-TotalInner         | 66.7           |
| dLoss                   | 0.0036359462   |
| n_timesteps             | 1520000        |
--------------------------------------------

 ---------------- Iteration 19 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 0
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 19            |
| ItrTime                 | 66.1          |
| LossAfter               | -0.008048734  |
| LossBefore              | 1.0608205e-09 |
| MeanKL                  | 0.009635925   |
| MeanKLBefore            | -1.974404e-08 |
| Step_0-AverageDiscou... | -16.9         |
| Step_0-AveragePolicyStd | 0.8929222     |
| Step_0-AverageReturn    | -33.5         |
| Step_0-EnvExecTime      | 8.83          |
| Step_0-MaxReturn        | -29.2         |
| Step_0-MinReturn        | -37.2         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.38          |
| Step_0-StdReturn        | 1.44          |
| Step_1-AverageDiscou... | -9.74         |
| Step_1-AveragePolicyStd | 0.8925615     |
| Step_1-AverageReturn    | -18.6         |
| Step_1-EnvExecTime      | 10.6          |
| Step_1-MaxReturn        | -14.2         |
| Step_1-MinReturn        | -26.4         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 32.3          |
| Step_1-StdReturn        | 2.32          |
| Time                    | 1.47e+03      |
| Time-InnerStep          | 0.0958        |
| Time-MAMLSteps          | 3.45          |
| Time-OuterStep          | 3.45          |
| Time-SampleProc         | 0.316         |
| Time-Sampling           | 62.2          |
| Time-TotalInner         | 62.6          |
| dLoss                   | 0.008048735   |
| n_timesteps             | 1600000       |
-------------------------------------------

 ---------------- Iteration 20 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 20             |
| ItrTime                 | 75.3           |
| LossAfter               | -0.0073316284  |
| LossBefore              | 3.1168264e-09  |
| MeanKL                  | 0.008677488    |
| MeanKLBefore            | -1.0430813e-08 |
| Step_0-AverageDiscou... | -16.7          |
| Step_0-AveragePolicyStd | 0.8890285      |
| Step_0-AverageReturn    | -32.9          |
| Step_0-EnvExecTime      | 10.9           |
| Step_0-MaxReturn        | -28.8          |
| Step_0-MinReturn        | -37.1          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 9.54           |
| Step_0-StdReturn        | 1.34           |
| Step_1-AverageDiscou... | -9.46          |
| Step_1-AveragePolicyStd | 0.88870984     |
| Step_1-AverageReturn    | -17.8          |
| Step_1-EnvExecTime      | 14.5           |
| Step_1-MaxReturn        | -14            |
| Step_1-MinReturn        | -24.1          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 33.7           |
| Step_1-StdReturn        | 1.73           |
| Time                    | 1.54e+03       |
| Time-InnerStep          | 0.0835         |
| Time-MAMLSteps          | 3.66           |
| Time-OuterStep          | 3.66           |
| Time-SampleProc         | 0.345          |
| Time-Sampling           | 71.1           |
| Time-TotalInner         | 71.6           |
| dLoss                   | 0.0073316316   |
| n_timesteps             | 1680000        |
--------------------------------------------

 ---------------- Iteration 21 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 2
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 21            |
| ItrTime                 | 68.9          |
| LossAfter               | -0.004735465  |
| LossBefore              | 2.542067e-10  |
| MeanKL                  | 0.009030347   |
| MeanKLBefore            | 1.3457611e-08 |
| Step_0-AverageDiscou... | -16.4         |
| Step_0-AveragePolicyStd | 0.8729991     |
| Step_0-AverageReturn    | -32.4         |
| Step_0-EnvExecTime      | 8.82          |
| Step_0-MaxReturn        | -28.9         |
| Step_0-MinReturn        | -36.3         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 8.45          |
| Step_0-StdReturn        | 1.22          |
| Step_1-AverageDiscou... | -9.35         |
| Step_1-AveragePolicyStd | 0.8732647     |
| Step_1-AverageReturn    | -17.7         |
| Step_1-EnvExecTime      | 10.2          |
| Step_1-MaxReturn        | -13.4         |
| Step_1-MinReturn        | -22.9         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 33.7          |
| Step_1-StdReturn        | 1.78          |
| Time                    | 1.61e+03      |
| Time-InnerStep          | 0.0852        |
| Time-MAMLSteps          | 5.12          |
| Time-OuterStep          | 5.12          |
| Time-SampleProc         | 0.318         |
| Time-Sampling           | 63.3          |
| Time-TotalInner         | 63.7          |
| dLoss                   | 0.0047354656  |
| n_timesteps             | 1760000       |
-------------------------------------------

 ---------------- Iteration 22 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 3
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
-------------------------------------------
| Itr                     | 22            |
| ItrTime                 | 70.1          |
| LossAfter               | -0.0072975555 |
| LossBefore              | -2.867764e-09 |
| MeanKL                  | 0.009244299   |
| MeanKLBefore            | -7.450581e-09 |
| Step_0-AverageDiscou... | -16           |
| Step_0-AveragePolicyStd | 0.8573869     |
| Step_0-AverageReturn    | -31.5         |
| Step_0-EnvExecTime      | 10.5          |
| Step_0-MaxReturn        | -27.8         |
| Step_0-MinReturn        | -34.8         |
| Step_0-NumTrajs         | 280           |
| Step_0-PolicyExecTime   | 9.02          |
| Step_0-StdReturn        | 1.31          |
| Step_1-AverageDiscou... | -9.25         |
| Step_1-AveragePolicyStd | 0.8574641     |
| Step_1-AverageReturn    | -17.4         |
| Step_1-EnvExecTime      | 10.5          |
| Step_1-MaxReturn        | -13.8         |
| Step_1-MinReturn        | -22.9         |
| Step_1-NumTrajs         | 280           |
| Step_1-PolicyExecTime   | 33.5          |
| Step_1-StdReturn        | 1.6           |
| Time                    | 1.68e+03      |
| Time-InnerStep          | 0.083         |
| Time-MAMLSteps          | 3.63          |
| Time-OuterStep          | 3.63          |
| Time-SampleProc         | 0.345         |
| Time-Sampling           | 66            |
| Time-TotalInner         | 66.5          |
| dLoss                   | 0.0072975527  |
| n_timesteps             | 1840000       |
-------------------------------------------

 ---------------- Iteration 23 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
backtrack iters: 1
computing loss after
optimization finished
Computing loss after
Computing KL after
Saving snapshot...
Saved
--------------------------------------------
| Itr                     | 23             |
| ItrTime                 | 66.4           |
| LossAfter               | -0.005430224   |
| LossBefore              | -1.085656e-09  |
| MeanKL                  | 0.007762243    |
| MeanKLBefore            | -2.9802323e-09 |
| Step_0-AverageDiscou... | -16.4          |
| Step_0-AveragePolicyStd | 0.8341704      |
| Step_0-AverageReturn    | -32.3          |
| Step_0-EnvExecTime      | 8.63           |
| Step_0-MaxReturn        | -29.2          |
| Step_0-MinReturn        | -36.4          |
| Step_0-NumTrajs         | 280            |
| Step_0-PolicyExecTime   | 8.55           |
| Step_0-StdReturn        | 1.21           |
| Step_1-AverageDiscou... | -9.07          |
| Step_1-AveragePolicyStd | 0.83409977     |
| Step_1-AverageReturn    | -17            |
| Step_1-EnvExecTime      | 10.3           |
| Step_1-MaxReturn        | -13.7          |
| Step_1-MinReturn        | -24.2          |
| Step_1-NumTrajs         | 280            |
| Step_1-PolicyExecTime   | 32.6           |
| Step_1-StdReturn        | 1.75           |
| Time                    | 1.75e+03       |
| Time-InnerStep          | 0.0819         |
| Time-MAMLSteps          | 3.71           |
| Time-OuterStep          | 3.71           |
| Time-SampleProc         | 0.347          |
| Time-Sampling           | 62.2           |
| Time-TotalInner         | 62.6           |
| dLoss                   | 0.005430223    |
| n_timesteps             | 1920000        |
--------------------------------------------

 ---------------- Iteration 24 ----------------
Sampling set of tasks/goals for this meta-batch...
** Step 0 **
Obtaining samples...
Processing samples...
Computing inner policy updates...
** Step 1 **
Obtaining samples...
Processing samples...
Optimizing policy...
Computing KL before
Computing loss before
Optimizing
Start CG optimization
computing loss before
performing update
computing gradient
gradient computed
computing descent direction
descent direction computed
